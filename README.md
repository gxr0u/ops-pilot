# OpsPilot â€“ AI Ops Assistant

OpsPilot is a **local-first, cost-aware AI Ops Assistant** that converts unstructured operational data  
(meeting notes, internal docs, Slack exports) into **structured decisions, action items, and risks**.

It is designed as a **production-style LLM orchestration system**, not a chatbot â€” emphasizing
deterministic outputs, strict validation, and selective use of large models.

---

## ğŸš€ Why OpsPilot?

Teams often rely on:
- messy meeting notes
- undocumented decisions
- manual follow-ups

OpsPilot turns raw operational text into **clear, structured, and actionable outputs** that teams can
immediately use.

Unlike generic chatbots, OpsPilot focuses on:
- reliability
- cost-awareness
- schema enforcement
- real-world deployment patterns

---

## âœ¨ Key Features

- ğŸ  **Local-first** (Ollama supported)
- ğŸ’° **Cost-aware model routing**
- ğŸ” **Two-pass LLM orchestration**
- ğŸ“ **Strict schema validation (Pydantic)**
- ğŸ§ª **Confidence scoring & fallback reasoning**
- ğŸ–¥ï¸ **Gradio UI for fast demos**

---
## ğŸ§  How It Works

OpsPilot uses a **multi-stage pipeline** where large language models are used **only when necessary**.

```text
Unstructured Input  
â†“  
Extraction Pass (Cheap / Local LLM)  
â†“  
Normalization & Cleanup  
â†“  
Confidence Scoring (Code)  
â†“  
Conditional Reasoning (Stronger LLM)  
â†“  
Schema Validation (Pydantic)  
â†“  
Structured Ops Output (JSON)
```
---
## ğŸ¤– Model Strategy

| Stage        | Model Used                          |
|--------------|-------------------------------------|
| Extraction   | Ollama (LLaMA 3.2 3B)               |
| Reasoning    | Groq / HuggingFace (Larger models)  |
| Validation   | Code (Pydantic)                     |

> **Enterprise / subscription deployments** can swap in GPT-4 or Claude at the routing layer  
> without changing the core architecture.
---
## ğŸ—ï¸ Project Structure
```text
ops-pilot/
â”œâ”€â”€ core/               # Shared infrastructure (model routing, schemas)
â”‚   â”œâ”€â”€ confidence.py
â”‚   â”œâ”€â”€ model_router.py
â”‚   â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ ops_assistant/      # Ops Assistant domain logic
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ extractor.txt
â”‚   â”‚   â””â”€â”€ reasoner.txt
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ service.py
â”‚
â”œâ”€â”€ ui/                 # Presentation layer
â”‚   â””â”€â”€ gradio_app.py
â”‚
â”œâ”€â”€ examples/           # Sample inputs & outputs
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```
---
## ğŸ“‚ Example

### Input (examples/meeting_notes.txt)

    We discussed the Q3 launch timeline.
    Decision was made to delay the launch by one week.
    Aditya will follow up with the frontend team.
    Risk identified around analytics vendor dependency.

### Output (examples/output.json)

    {
      "decisions": ["delay Q3 launch by one week"],
      "action_items": [
        {
          "task": "follow up with the frontend team",
          "owner": "Aditya",
          "priority": "HIGH"
        }
      ],
      "risks": ["analytics vendor dependency"]
    }
---
## â–¶ï¸ Running Locally

### 1. Install dependencies

    pip install -r requirements.txt

### 2. Start Ollama

    ollama run llama3.2:3b

### 3. Launch the Gradio UI

    python -m ui.gradio_app

Open in your browser:

    http://localhost:7860
---
## ğŸ” Environment Variables

Create a `.env` file using `.env.example` as reference:

    GROQ_API_KEY=your_groq_api_key
    HF_API_KEY=your_huggingface_api_key
    OLLAMA_URL=http://localhost:11434

âš ï¸ Never commit your `.env` file.
---
